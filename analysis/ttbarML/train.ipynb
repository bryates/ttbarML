{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: tqdm in /tmpscratch/byates2/.local/lib/python3.8/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "!pip install tqdm\n",
    "#!pip install \"coffea==0.7.22\"\n",
    "#!pip install \"numpy>=1.22\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "#from analysis.ttbarML.options import handleOptions\n",
    "from options import handleOptions\n",
    "#from utils.options import handleOptions\n",
    "#from analysis.ttbarML.metrics import net_eval\n",
    "from metrics import net_eval\n",
    "#from utils.metrics import net_eval\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mport torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_plot(net, loss_test, loss_train, label, test_input, test_feat, feature_division, norm_test, norm_targ, show=False):\n",
    "    '''\n",
    "    creates a directory with the label name and saves the network, the loss plot, the network output plot, the ROC curve and the performance metrics\n",
    "        net: neural network\n",
    "        loss_test: loss on the test dataset\n",
    "        loss_train: loss on the training dataset\n",
    "        label: name of the directory\n",
    "        bsm_name: name of the BSM term\n",
    "        test: test dataset\n",
    "    '''\n",
    "    try:\n",
    "        os.mkdir(f'{label}')\n",
    "    except:\n",
    "        pass\n",
    "    torch.save(net, f'{label}/network.p')\n",
    "    torch.save(net.state_dict(), f'{label}/network_state_dict.p')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8,8])\n",
    "    \n",
    "    ax.plot( range(len(loss_test)), loss_train, label=\"Training dataset\")\n",
    "    ax.plot( range(len(loss_test)), loss_test , label=\"Testing dataset\")\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/loss.png')\n",
    "    ax.set_yscale('log')\n",
    "    fig.savefig(f'{label}/loss_log.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    \n",
    "    #print(f'Input values {test_input=}')\n",
    "    #print(f'Expected values {test_feat=}')\n",
    "    #print(f'Output values {net(test_feat)=}')\n",
    "    #print(test[0].detach().cpu().numpy(), net(test[:]).ravel().detach().cpu().numpy())\n",
    "    #sm_hist,bins,_  = ax.hist(net(test[0][3:]).ravel().detach().cpu().numpy(),\n",
    "    #print('plotting features', test_feat.detach().cpu().numpy()[0])\n",
    "    #print('plotting features', test_feat[:,0].detach().cpu().numpy()[0])\n",
    "    #print('plotting inputs',   test_input[:,0].detach().cpu().numpy()[0])\n",
    "    #print('plotting inputs',   test_input[0].detach().cpu().numpy()[0])\n",
    "    #print('plotting outputs',  net(test_input).detach().cpu().numpy()[:,0][0])\n",
    "    #print('plotting outputs',  net(test_input).detach().cpu().numpy())\n",
    "    #print(net(test_input[0]).shape)\n",
    "    #print('plotting predictions', net(test_input[0]).detach().cpu().numpy())\n",
    "    #print('Plotting x,y', test_feat[0].detach().cpu().numpy()*norm_targ[0].detach().cpu().numpy(), net(test).ravel().detach().cpu().numpy()*norm_test.detach().cpu().numpy()[0])\n",
    "    #print('Plotting x,y', test_feat[0].detach().cpu().numpy()*norm_targ[0].detach().cpu().numpy(), net(test).ravel().detach().cpu().numpy()[::feature_division]*norm_test.detach().cpu().numpy()[0])\n",
    "    #print(len(test_feat[:,0].detach().cpu().numpy()*norm_targ[0].detach().cpu().numpy()), len(net(test).ravel().detach().cpu().numpy()[:,0]*norm_test.detach().cpu().numpy()[0]))\n",
    "    #print(test_feat.shape)\n",
    "    #for i in range(min(10, test_feat.shape[0])):\n",
    "    #for i in range(test_feat.shape[0]):\n",
    "        #sm_hist  = ax.scatter(test_feat[i].detach().cpu().numpy()[0]*norm_targ[0].detach().cpu().numpy()[0], net(test_input[i]).detach().cpu().numpy()[0]*norm_test.detach().cpu().numpy()[0][0],\n",
    "        #print(f'Plotting {i} {test_feat[i].detach().cpu().numpy(), net(test_input[i]).detach().cpu().numpy()}')\n",
    "        #sm_hist  = ax.scatter(test_feat[i].detach().cpu().numpy(), net(test_input[i]).detach().cpu().numpy(),\n",
    "    #sm_hist  = ax.scatter(test[0][0].detach().cpu().numpy(), net(test[0][3:]).ravel().detach().cpu().numpy()[0],\n",
    "    #sm_hist,bins,_  = ax.scatter(test[0][0].detach().cpu().numpy(), net(test[0][3:]).ravel().detach().cpu().numpy()[0],\n",
    "                           #weights=test[0][3:][0].detach().cpu().numpy(),\n",
    "                           #alpha=0.5, label='SM')\n",
    "                           #bins=100, alpha=0.5, label='SM', density=True)\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,0], net(test_input).detach().cpu().numpy()[:,0],\n",
    "                          alpha=0.5, label='px')\n",
    "    '''\n",
    "    '''\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,1], net(test_input).detach().cpu().numpy()[:,1],\n",
    "                          alpha=0.5, label='py')\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,2], net(test_input).detach().cpu().numpy()[:,2],\n",
    "                          alpha=0.5, label='pz')\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,3], net(test_input).detach().cpu().numpy()[:,3],\n",
    "                          alpha=0.5, label='e')\n",
    "\n",
    "    ax.set_xlabel('Target Output', fontsize=12)\n",
    "    ax.set_ylabel('Network Output', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_out.png')\n",
    "    plt.clf()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    bins = np.linspace(-0.5, 0.5, 10)\n",
    "    bins = np.array([bins, bins])\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,0], net(test_input).detach().cpu().numpy()[:,0],\n",
    "                          alpha=0.5, label='px')\n",
    "\n",
    "    ax.set_xlabel('Target Output', fontsize=12)\n",
    "    ax.set_ylabel('Network Output', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_out_px.png')\n",
    "    plt.clf()\n",
    "    '''\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,1], net(test_input).detach().cpu().numpy()[:,1],\n",
    "                          alpha=0.5, label='py')\n",
    "\n",
    "    ax.set_xlabel('Target Output', fontsize=12)\n",
    "    ax.set_ylabel('Network Output', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_out_py.png')\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,2], net(test_input).detach().cpu().numpy()[:,2],\n",
    "                          alpha=0.5, label='pz')\n",
    "\n",
    "    ax.set_xlabel('Target Output', fontsize=12)\n",
    "    ax.set_ylabel('Network Output', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_out_pz.png')\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    sm_hist  = ax.scatter(test_feat.detach().cpu().numpy()[:,3], net(test_input).detach().cpu().numpy()[:,3],\n",
    "                          alpha=0.5, label='e')\n",
    "\n",
    "    ax.set_xlabel('Target Output', fontsize=12)\n",
    "    ax.set_ylabel('Network Output', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_out_energy.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    \n",
    "    #for i in range(min(10, test_feat.shape[0])):\n",
    "        #sm_hist  = ax.hist(test_feat[i].detach().cpu().numpy()[0]*norm_targ[0].detach().cpu().numpy()[0] - net(test_input[i]).detach().cpu().numpy()[0]*norm_test.detach().cpu().numpy()[0][0],\n",
    "        #sm_hist  = ax.hist(test_feat[i].detach().cpu().numpy()[0] - net(test_input[i]).detach().cpu().numpy()[0],\n",
    "        #                    bins=res_bins, alpha=0.5, label='SM')\n",
    "    #print('target', test_feat.detach().cpu().numpy()[:,0], 'pred', net(test_input).detach().cpu().numpy()[:,0], test_feat.detach().cpu().numpy()[:,0] - net(test_input).detach().cpu().numpy()[:,0])\n",
    "        #print(i,\n",
    "        #      f\"{test_feat.detach().cpu().numpy()[:,0]=}\",\n",
    "        #      f\"{test_feat[0].detach().cpu().numpy()[0]=}\",#*norm_targ[0].detach().cpu().numpy()[0]=}\",\n",
    "        #      f\"{net(test_input[i]).detach().cpu().numpy()[0]=}\",#*norm_test.detach().cpu().numpy()[0][0]=}\",\n",
    "        #      f\"{test_feat[i].detach().cpu().numpy()[0]*norm_targ[0].detach().cpu().numpy()[0] - net(test_input[i]).detach().cpu().numpy()[0]*norm_test.detach().cpu().numpy()[0][0]=}\")\n",
    "    res = test_feat.detach().cpu().numpy() - net(test_input).detach().cpu().numpy()\n",
    "    res_bins = np.linspace(np.min(res), np.max(res), 100)\n",
    "    res_bins = np.linspace(-1, 1, 100)\n",
    "    sm_hist  = ax.hist(res,\n",
    "                       bins=res_bins, alpha=0.5, label=['px','py','pz','e'])\n",
    "    ax.set_xlabel('Residual', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_res.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    res = net(test_input).detach().cpu().numpy() / test_feat.detach().cpu().numpy()\n",
    "    res_bins = np.linspace(-50, 50, 100)\n",
    "    sm_hist  = ax.hist(res,\n",
    "                       bins=res_bins, alpha=0.5, label=['px','py','pz','e'])\n",
    "    ax.set_xlabel('out/targ', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/net_pval.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    res = net(test_input).detach().cpu().numpy()[:,0]\n",
    "    targ = test_feat.detach().cpu().numpy()[:,0]\n",
    "    res_bins = np.linspace(np.min(res), np.max(res), 25)\n",
    "    #sm_hist  = ax.hist([net(test_input).detach().cpu().numpy()[:,0], test_feat.detach().cpu().numpy()[:,0]],\n",
    "    #                   bins=res_bins, alpha=0.5, label=['pred', 'gen'], alpha=0.5)\n",
    "    sm_hist  = ax.hist(net(test_input).detach().cpu().numpy()[:,0],\n",
    "                       bins=res_bins, alpha=0.5, label='pred')\n",
    "    sm_hist  = ax.hist(test_feat.detach().cpu().numpy()[:,0],\n",
    "                       bins=res_bins, alpha=0.5, label='gen')\n",
    "    ax.set_xlabel('px', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/comp_x.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    printf(f'R^2 x: {np.sqrt(np.sum(np.squar(targ - res)))}')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    res = net(test_input).detach().cpu().numpy()[:,1]\n",
    "    targ = test_feat.detach().cpu().numpy()[:,1]\n",
    "    res_bins = np.linspace(np.min(res), np.max(res), 25)\n",
    "    #sm_hist  = ax.hist([net(test_input).detach().cpu().numpy()[:,0], test_feat.detach().cpu().numpy()[:,0]],\n",
    "    #                   bins=res_bins, alpha=0.5, label=['pred', 'gen'], alpha=0.5)\n",
    "    sm_hist  = ax.hist(net(test_input).detach().cpu().numpy()[:,1],\n",
    "                       bins=res_bins, alpha=0.5, label='pred')\n",
    "    sm_hist  = ax.hist(test_feat.detach().cpu().numpy()[:,1],\n",
    "                       bins=res_bins, alpha=0.5, label='gen')\n",
    "    ax.set_xlabel('py', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/comp_y.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    printf(f'R^2 y: {np.sqrt(np.sum(np.squar(targ - res)))}')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    res = net(test_input).detach().cpu().numpy()[:,2]\n",
    "    targ = test_feat.detach().cpu().numpy()[:,2]\n",
    "    res_bins = np.linspace(np.min(res), np.max(res), 25)\n",
    "    #sm_hist  = ax.hist([net(test_input).detach().cpu().numpy()[:,0], test_feat.detach().cpu().numpy()[:,0]],\n",
    "    #                   bins=res_bins, alpha=0.5, label=['pred', 'gen'], alpha=0.5)\n",
    "    sm_hist  = ax.hist(net(test_input).detach().cpu().numpy()[:,2],\n",
    "                       bins=res_bins, alpha=0.5, label='pred')\n",
    "    sm_hist  = ax.hist(test_feat.detach().cpu().numpy()[:,2],\n",
    "                       bins=res_bins, alpha=0.5, label='gen')\n",
    "    ax.set_xlabel('pz', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/comp_z.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    printf(f'R^2 z: {np.sqrt(np.sum(np.squar(targ - res)))}')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[12,7])\n",
    "    res = net(test_input).detach().cpu().numpy()[:,3]\n",
    "    targ = test_feat.detach().cpu().numpy()[:,3]\n",
    "    res_bins = np.linspace(np.min(res), np.max(res), 25)\n",
    "    #sm_hist  = ax.hist([net(test_input).detach().cpu().numpy()[:,0], test_feat.detach().cpu().numpy()[:,0]],\n",
    "    #                   bins=res_bins, alpha=0.5, label=['pred', 'gen'], alpha=0.5)\n",
    "    sm_hist  = ax.hist(net(test_input).detach().cpu().numpy()[:,3],\n",
    "                       bins=res_bins, alpha=0.5, label='pred')\n",
    "    sm_hist  = ax.hist(test_feat.detach().cpu().numpy()[:,3],\n",
    "                       bins=res_bins, alpha=0.5, label='gen')\n",
    "    ax.set_xlabel('energy', fontsize=12)\n",
    "    ax.legend()\n",
    "    if show: plt.show()\n",
    "    fig.savefig(f'{label}/comp_energy.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    printf(f'R^2 energy: {np.sqrt(np.sum(np.squar(targ - res)))}')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    '''\n",
    "    roc, auc, a = net_eval(net(test_input[:]), test_feat[:], test_feat[:])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8,8])\n",
    "    ax.plot(roc[:,0], roc[:,1], label='Network Performance')\n",
    "    ax.plot([0,1],[0,1], ':', label='Baseline')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=14)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=14)\n",
    "    fig.savefig(f'{label}/ROC.png')\n",
    "    if show: plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8,8])\n",
    "    ax.plot(roc[:,0], roc[:,1], label='Network Performance')\n",
    "    ax.plot([0,1],[0,1], ':', label='Baseline')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=14)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=14)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    fig.savefig(f'{label}/ROC_log.png')\n",
    "    if show: plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.close()\n",
    "    auc = auc.data.cpu().numpy()\n",
    "    a   = a.data.cpu().numpy()\n",
    "    \n",
    "    f = open(f'{label}/performance.txt','w+')\n",
    "    f.write(f'Area under ROC: {auc}\\nAccuracy:       {a}\\n')\n",
    "    f.close()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1_px,top1_py,top1_pz,top1_energy,top2_px,top2_py,top2_pz,top2_energy,lepton1_px,lepton1_py,lepton1_pz,lepton1_energy,lepton2_px,lepton2_py,lepton2_pz,lepton2_energy,jet1_px,jet1_py,jet1_pz,jet1_energy,jet2_px,jet2_py,jet2_pz,jet2_energy,jet3_px,jet3_py,jet3_pz,jet3_energy,jet4_px,jet4_py,jet4_pz,jet4_energy,jet5_px,jet5_py,jet5_pz,jet5_energy,jet6_px,jet6_py,jet6_pz,jet6_energy,jet7_px,jet7_py,jet7_pz,jet7_energy,jet8_px,jet8_py,jet8_pz,jet8_energy,met_px,met_py\n",
      "device='cuda'\n",
      "Will redo tensor with input features\n",
      "Loading files, this may take a while\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1463330/107036284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1463330/107036284.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m#from analysis.ttbarML.data import eftDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meftDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0msignal_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meftDataLoader\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;31m#sm_mean = torch.mean(signal_dataset.sm_weight); bsm_mean = torch.mean(signal_dataset.bsm_weight)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch365/byates2/ttbarML/analysis/ttbarML/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch365/byates2/ttbarML/analysis/ttbarML/data.py\u001b[0m in \u001b[0;36mbuild_tensors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             '''\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m#tf = uproot.open( fil )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfil\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;31m#events = NanoEventsFactory.from_root(fil, schemaclass=NanoAODSchema).events()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m#print(len(events))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             result = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         dataset = ParquetDataset(\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             self._dataset = ds.FileSystemDataset(\n\u001b[0;32m-> 1360\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "# Register hooks for all ReLU layers\n",
    "activations = []\n",
    "\n",
    "def activation_hook(module, input, output):\n",
    "    activations.append(output)\n",
    "        \n",
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    #args = handleOptions()\n",
    "    #parser.add_argument(\"--reload\",  action='store_true', default=False, help=\"Force conversion of hdf to pytorch\")\n",
    "    args = {}\n",
    "    files = \"top_quark_reco_events_NAOD-00000_89*\"\n",
    "    files = \"top_quark_reco_events_NAOD-00000_898.root\"\n",
    "    #files = \"top_quark_reco_events_NAOD-00000_*\"\n",
    "    #files = \"top_quark_reco_events.root\"\n",
    "    #files = \"NAOD-00000_1016.root\"\n",
    "    device = 'cuda'\n",
    "    name = ''\n",
    "    out_path = '.'\n",
    "    features = \"ttbar_pt, ttbar_eta, ttbar_phi, ttbar_pz, ttbar_mass, ttbar_energy, Lep1_pt,Lep2_pt,Lep1_eta,Lep2_eta,Lep2_phi,Lep2_phi,Jet1_pt,Jet1_eta,Jet1_phi,Jet2_pt,Jet2_eta,Jet2_phi,Jet3_pt,Jet3_eta,Jet3_phi,Jet4_pt,Jet4_eta,Jet4_phi,met_pt,PT_pt\"\n",
    "    features = \"top1_pt,lep1_pt\"\n",
    "    input_features = \"ttbar_px,ttbar_py,ttbar_pz,ttbar_energy\"\n",
    "    input_features = \"top1_px,top1_py,top1_pz,top1_energy,top2_px,top2_py,top2_pz,top2_energy\"\n",
    "    #input_features = \"ttbar_px\"\n",
    "    input_nodes = \"lepton1_px,lepton1_py,lepton1_pz,lepton1_energy,lepton2_px,lepton2_py,lepton2_pz,lepton2_energy,jet1_px,jet1_py,jet1_pz,jet1_energy,jet2_px,jet2_py,jet2_pz,jet2_energy,jet3_px,jet3_py,jet3_pz,jet3_energy,jet4_px,jet4_py,jet4_pz,jet4_energy,met_px,met_py\"\n",
    "    input_nodes = \"lepton1_px,lepton1_py,lepton1_pz,lepton1_energy,lepton2_px,lepton2_py,lepton2_pz,lepton2_energy,jet1_px,jet1_py,jet1_pz,jet1_energy,jet2_px,jet2_py,jet2_pz,jet2_energy,jet3_px,jet3_py,jet3_pz,jet3_energy,jet4_px,jet4_py,jet4_pz,jet4_energy,jet5_px,jet5_py,jet5_pz,jet5_energy,jet6_px,jet6_py,jet6_pz,jet6_energy,jet7_px,jet7_py,jet7_pz,jet7_energy,jet8_px,jet8_py,jet8_pz,jet8_energy,met_px,met_py\"\n",
    "    features = ','.join([input_features, input_nodes])\n",
    "    print(features)\n",
    "    #features = \"ttbar_x, ttbar_y, ttbar_z, Lep1_x,Lep2_x,Lep1_y,Lep2_y,Lep2_z,Lep2_z,Jet1_x,Jet1_y,Jet1_z,Jet2_x,Jet2_y,Jet2_z,Jet3_x,Jet3_y,Jet3_z,Jet4_x,Jet4_y,Jet4_z,met_x\"\n",
    "    feature_division = len(input_features.split(\",\"))\n",
    "    forceRebuild = True\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    learning_rate = 0.0001\n",
    "    factor = 0.01\n",
    "    patience = 10\n",
    "    norm = True\n",
    "    profile = False\n",
    "    args['files'] = files\n",
    "    args['device'] = device\n",
    "    args['name'] = name\n",
    "    args['out_path'] = out_path\n",
    "    args['features'] = features\n",
    "    args['forceRebuild'] = forceRebuild\n",
    "    args['batch_size'] = batch_size\n",
    "    args['epochs'] = epochs\n",
    "    args['learning_rate'] = learning_rate\n",
    "    args['factor'] = factor\n",
    "    args['patience'] = patience\n",
    "    args['norm'] = norm\n",
    "    args['profile'] = profile\n",
    "\n",
    "    # Now we decide how (if) we will use the gpu\n",
    "    if device != 'cpu' and not torch.cuda.is_available():\n",
    "        print(\"Warning, you tried to use cuda, but its not available. Will use the CPU\")\n",
    "        device = 'cpu'\n",
    "    print(f'{device=}')\n",
    "\n",
    "    # If we use the cpu we dont use the whole UI (at psi)\n",
    "    torch.set_num_threads(8)\n",
    "\n",
    "    # all the stuff below should be configurable in the future\n",
    "    # we get the model = net + cost function\n",
    "    #from analysis.ttbarML.net import Model\n",
    "    from net import Model\n",
    "    #from models.net import Model\n",
    "    model = Model(features = len(features.split(\",\")), feature_division = feature_division, device = device)\n",
    "        \n",
    "    # Register hooks for all ReLU layers\n",
    "    for layer in model.net.modules():\n",
    "        if isinstance(layer, nn.ReLU) or isinstance(layer, nn.LeakyReLU):\n",
    "            layer.register_forward_hook(activation_hook)\n",
    "\n",
    "    # now we get the data\n",
    "    #from analysis.ttbarML.data import eftDataLoader\n",
    "    from data import eftDataLoader\n",
    "    signal_dataset = eftDataLoader( args )\n",
    "    #sm_mean = torch.mean(signal_dataset.sm_weight); bsm_mean = torch.mean(signal_dataset.bsm_weight)\n",
    "    mean = torch.mean(signal_dataset.features)\n",
    "    dataset_size = len(signal_dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    train, test    = torch.utils.data.random_split( signal_dataset, [train_size, (dataset_size-train_size)], generator=torch.Generator().manual_seed(42))\n",
    "    train, test    = torch.utils.data.random_split( signal_dataset, [train_size, (dataset_size-train_size)], generator=torch.Generator().manual_seed(0))\n",
    "    #dataloader     = DataLoader(  train  , batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "    #normalize features\n",
    "    mu,std = torch.mean(train[:]), torch.std(train[:])\n",
    "    norm_train_targ, norm_train = train[:][:,:feature_division].norm(p=2, dim=1, keepdim=True), train[:][:,feature_division:].norm(p=2, dim=1, keepdim=True)\n",
    "    #train_targ, train = torch.nn.functional.normalize(train[:][:,:feature_division],dim=1), torch.nn.functional.normalize(train[:][:,feature_division:],dim=1)\n",
    "    train_targ, train_input = train[:][:,:feature_division], train[:][:,feature_division:]\n",
    "    #train_targ, train_input = torch.nn.functional.normalize(train_targ[:],dim=1), torch.nn.functional.normalize(train_input[:],dim=1)\n",
    "    #train_targ  = train_targ[:100]\n",
    "    #train_input = train_input[:100]\n",
    "    \n",
    "    if not args['norm']:\n",
    "        train_targ = train[:][:,:feature_division]\n",
    "        train_input = train[:][:,feature_division:]\n",
    "        norm_train_targ, norm_train_input = torch.ones_like(norm_train_targ), torch.ones_like(norm_train_targ)\n",
    "    norm_test_targ, norm_test = test[:][:,:feature_division].norm(p=2, dim=1, keepdim=True), test[:][:,feature_division:].norm(p=2, dim=1, keepdim=True)\n",
    "    #test_targ, test = torch.nn.functional.normalize(test[:][:,:feature_division],dim=1), torch.nn.functional.normalize(test[:][:,feature_division:],dim=1)\n",
    "    test_targ, test_input = test[:][:,:feature_division], test[:][:,feature_division:]\n",
    "    #test_targ, test_input = torch.nn.functional.normalize(test_targ[:],dim=1), torch.nn.functional.normalize(test_input[:],dim=1)\n",
    "    \n",
    "    #train_targ = (train_targ - torch.mean(train_targ)) / torch.std(train_targ)\n",
    "    #train_input = (train_input - torch.mean(train_input)) / torch.std(train_input)\n",
    "    #test_targ = (test_targ - torch.mean(test_targ)) / torch.std(test_targ)\n",
    "    #test_input = (test_input - torch.mean(test_input)) / torch.std(test_input)\n",
    "\n",
    "    if not args['norm']:\n",
    "        test_targ = test[:][:,:feature_division]\n",
    "        test_input = test[:][:,feature_division:]\n",
    "        norm_test_targ, norm_test_input = torch.ones_like(norm_test_targ), torch.ones_like(norm_test_targ)\n",
    "        \n",
    "    train_targ_mean, train_targ_std = torch.mean(train_targ.T, dim=1).T, torch.std(train_targ.T, dim=1).T\n",
    "    train_input_mean, train_input_std = torch.mean(train_input.T, dim=1).T, torch.std(train_input.T, dim=1).T\n",
    "    test_targ_mean, test_targ_std = torch.mean(test_targ.T, dim=1).T, torch.std(test_targ.T, dim=1).T\n",
    "    test_input_mean, test_input_std = torch.mean(test_input.T, dim=1).T, torch.std(test_input.T, dim=1).T\n",
    "    train_targ = (train_targ - train_targ_mean) / train_targ_std\n",
    "    train_input = (train_input - train_input_mean) / train_input_std\n",
    "    test_targ = (test_targ - test_targ_mean) / test_targ_std\n",
    "    test_input = (test_input - test_input_mean) / test_input_std\n",
    "\n",
    "    # Put back energy mean to avoid bias\n",
    "    #train_targ[:,3] = train_targ[:,3] + train_targ_mean[3] / train_targ_std[3]\n",
    "    #train_targ[:,3] = train_input[:,3] + train_input_mean[3] / train_input_std[3]\n",
    "    #test_targ[:,3] = test_targ[:,3] + test_targ_mean[3] / test_targ_std[3]\n",
    "    #test_input[:,3] = test_input[:,3] + test_input_mean[3] / test_input_std[3]\n",
    "    \n",
    "    dataloader     = DataLoader(  torch.cat([train_targ,train_input], dim=1)  , batch_size=args['batch_size'], shuffle=True)\n",
    "    #test  = torch.nn.functional.normalize(test[:])\n",
    "    optimizer = optim.Adam(model.net.parameters(), lr=args['learning_rate'])\n",
    "    #optimizer = optim.SGD(model.net.parameters(), lr=args.learning_rate, momentum=args.momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=args['factor'], patience=args['patience'])\n",
    "    loss_train = [model.cost_from_batch(train_targ, train_input, args['device']).item()]\n",
    "    #loss_train = [model.cost_from_batch(train[:,feature_division:], train[:,0:feature_division] , args.device).item()]\n",
    "    loss_test  = [model.cost_from_batch(test_targ , test_input,  args['device']).item()]\n",
    "    dead = []\n",
    "    print([x/1024/1024/1024 for x in torch.cuda.mem_get_info()])\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "        #for i,(tops, features) in enumerate(dataloader):\n",
    "        for i,(features) in enumerate(dataloader):\n",
    "            #print(f'{len(features)}')\n",
    "            #print(f'Training {epoch=} on {features}')\n",
    "            targets = features[:,0:feature_division]\n",
    "            features = features[:,feature_division:]\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.cost_from_batch(targets, features, args['device'])\n",
    "            if args['profile']:\n",
    "                dead.append([((activation.abs() < 0.01).sum().item(), activation.numel()) for activation in activations])\n",
    "                #for i, activation in enumerate(activations):\n",
    "                #    # Count number of dead neurons (all zeros)\n",
    "                #    #dead_neurons = (activation == 0).sum().item()\n",
    "                #    dead_neurons = (activation.abs() < 0.01).sum().item()\n",
    "                #    total_neurons = activation.numel()\n",
    "                #    dead.append((dead_neurons, total_neurons))\n",
    "                #    #print(f\"Layer {i+1}: {float(dead_neurons)/total_neurons*100}% dead neurons\")\n",
    "                ##print(loss.detach().cpu().numpy())\n",
    "                #print(f'Found {100 * np.sum([x for x,_ in dead[-1]]) / np.sum([y for _,y in dead[-1]])}% dead neurons!')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "        loss_train.append( model.cost_from_batch(train_targ, train_input , args['device']).item())\n",
    "        loss_test .append( model.cost_from_batch(test_targ , test_input,  args['device']).item())\n",
    "        scheduler.step(loss_train[epoch])\n",
    "        if epoch%50==0:\n",
    "            if args['profile']: print(f'Found {100 * np.sum([x for x,_ in dead[-1]]) / np.sum([y for _,y in dead[-1]])}% dead neurons!')\n",
    "            save_and_plot( model.net, loss_test, loss_train, f\"test_epoch_{epoch}\", test_input, test_targ, feature_division, norm_test, norm_test_targ, show=True)\n",
    "            #print(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "    save_and_plot( model.net, loss_test, loss_train, f\"test_last\", test_input, test_targ, feature_division, norm_test, norm_test_targ, show=True)\n",
    "    print(f'Found {100 * np.sum([x for x,_ in dead]) / np.sum([y for _,y in dead])}% dead neurons!')\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
